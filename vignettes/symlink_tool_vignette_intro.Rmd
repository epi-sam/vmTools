---
title: "Symlink Tool Intro Vignette"
output: 
  html_document:
    toc: true
    toc_float: true
# This is lighter weight, but doesn't support TOC
# output: rmarkdown::html_vignette
# vignette: >
#   %\VignetteIndexEntry{symlink_tool_vignette}
#   %\VignetteEngine{knitr::rmarkdown}
#   %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Chunk options

# This is a nice default so log output doesn't wrap
options(width = 320)

# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output
# if the output is too long, it will be truncated like:
# 
# top output
# ...
# bottom output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # truncate the output
      # x <- c(head(x, n), "....\n")
      x <- c(head(x, n/2), '....', tail(x, n/2 + 1))
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```


```{r setup, include = FALSE}
# Installing locally only allows main branch - switching to load_all for development
devtools::load_all()
# library(vmTools, lib.loc = file.path("/mnt/share/code", Sys.info()[["user"]], "r_pkgs"))
library(data.table)
library(fs)
```


```{r utils, include = FALSE}

# Defining a couple vignette utilities

# Print the folder output 'tree' structure
print_tree <- function(root_base) {fs::dir_tree(root_base, recurse = TRUE)}

# Make a directory with desired defaults without cluttering the vignette
make_directory <- function(path){
  dir.create(path, recursive = TRUE, showWarnings = FALSE)
}

# print a symlink's target from the file system 
print_symlink <- function(symlink_type, root_input){
  print(grep(symlink_type, system(paste("ls -alt", root_input), intern = TRUE), value = TRUE))
}

#' Get output directory for results to save in.
#'
#' Returns a path to save results in of the form "YYYY_MM_DD.VV".
#'
#' @param root path to root of output results
#' @param date character date in form of "YYYY_MM_DD" or "today". "today" will be interpreted as today's date.
get_output_dir <- function(root, date) {
  if (date == "today") {
    date <- format(Sys.Date(), "%Y_%m_%d")
  }
  cur.version <- get_latest_output_date_index(root, date = date)
  
  dir.name <- sprintf("%s.%02i", date, cur.version + 1)
  return(dir.name)
}

#' get the latest index for given an output dir and a date
#'
#' directories are assumed to be named in YYYY_MM_DD.VV format with sane
#' year/month/date/version values.
#'
#' @param dir path to directory with versioned dirs
#' @param date character in be YYYY_MM_DD format
#'
#' @return largest version in directory tree or 0 if there are no version OR
#' the directory tree does not exist
get_latest_output_date_index <- function(root, date) {
  currentfolders <- list.files(root)
  
  # subset to date
  pat <- sprintf("^%s[.]\\d{2}$", date)
  date_dirs <- grep(pat, currentfolders, value = TRUE)
  
  if (length(date_dirs) == 0) {
    return(0)
  }
  
  # get the index after day
  date_list <- strsplit(date_dirs, "[.]")
  
  inds <- unlist(lapply(date_list, function(x) x[2]))
  if (is.na(max(inds, na.rm = TRUE))) inds <- 0
  
  return(max(as.numeric(inds)))
}

resolve_symlink <- function(path){
   path_resolved <- system(paste("realpath", path), intern = TRUE)
   if(file.exists(path_resolved)) {
      return(path_resolved)
   } else {
      message("Could not resolve symlink: ", path)
   }
}
```




# What is the SymLink Tool?



## Give Me a High-Level Overview


The SymLink Tool is a way for researchers to manage multiple data pipeline output runs.  

- It's a device you can 'drop into' your pipeline to automatically track when a run is 'best' or 'important for a paper'.

It's designed with user flexibility and project officers in mind, and doesn't require using anything like a database.

- Everything important you need to see is immediately visible as files and folders in the file system.




## Is This for Me and My Team?

This tool assumes you have a large set of output folders for runs of your pipeline, and you store them on the file system.

- If you're already using a central database to manage your output versions, you probably don't need this.  You can stop reading now!




## Give Me a Little More Detail


The Symlink Tool will:

1. Create a 'best' symlink to the 'best' pipeline run of your outputs.
    1. This is just a shortcut link in the `H:` or `J:` drive i.e. on the File System.
    1. It allows you to have a **stable file path** to results **that you and others don't need to update**.
    1. Safely update the 'best' symlink when you, the researcher, want to 'promote' a new pipeline run to being 'best'.



> OK, that's nice, but symlinks are pretty easy to make.  What else do I (the researcher) get **for free** if I use this tool?


The most important thing you get are some simple **logs** that automatically keep track of which folders have been 'best.'

1. Maintain a **central log.**
    1. Which pipeline run is currently 'best'?
    1. Which older runs were _ever_ 'best' in case you need to 'roll back' what you consider 'best'.
    1. Track deletion of old pipeline runs.
1. Maintain **run-specific logs.**
    1. When was this run started?  (the file system _doesn't_ track this!)
    1. When was it marked 'best'?
    1. If it stopped being 'best', when did that happen?


> Hmm, is that all?  I feel like I could keep an excel document or HUB page that does the same thing?


That's very true, but this doesn't require you to remember, or do any typing yourself.  

Also, maybe there are pipeline runs you want to track for different reasons.  

- What if you want to keep a pipeline run for a paper, but it's not the 'best' GBD pipeline run.  What if you write multiple papers?  
- Do you want to keep track of all those by hand?  I don't!

1. Mark folders as 'keep', with a symlink and log entry that tells you it's important.
    1. You get all the same central log benefits as the 'best' symlink.
    

And your **Project Officer** gets things too!

1. Run reports on **all these logs simultaneously** about the status of all your important pipeline outputs.  



> That sounds pretty nice, but didn't you also say something about deleting folders?  Why do I need help doing that?


You get some additional benefits - The SymLink Tool will also:

1. Let you mark folders you want to 'remove' **before** you actually delete them.
    1. This is a **staging area** where you can take time to decide before you actually delete them.
    1. You can also 'unmark' them for removal if you change your mind.

When you're ready to delete, you'll get:
    1. **Safety** 
        - The Symlink Tool will only delete folders that marked to 'remove'.
    1. **Provenance** 
        - You'll get a record in the central log telling you which pipeline runs were deleted, when, why they were deleted (user gets to add a comment).



> I'm still reading, and curious to see how this works.





--------------------------------------------------------------------------------



# Demonstration 


In the following demonstration, we'll show the life-cycle of a pipeline run's
output folder, from creation, to filling with data, to marking as 'best', to
marking as 'keep', to marking as 'remove', and finally to deletion.

We'll show how the logs update at a couple key points.

This won't be an exhaustive demonstration of all the available options, this is a vignette of an average use case.

See the `symlink_tool_vignette_technical.Rmd` file for more detailed technical explanations.




## The `to_model` Folder

My team uses a `to_model` folder for all inputs we submit to ST-GPR.  
This way we can prepare the data, then submit various ST-GPR models with
different parameters without needing to re-prep the inputs. The results of the
ST-GPR models go into an output folder, which we'll ignore for simplicity.


```{r define_roots_invis, include = FALSE}
# Building up a dummy output folder

# Make the root folders
root_slt      <- file.path(tempdir(), "slt")
root_to_model <- file.path(root_slt, "to_model")
make_directory(root_to_model)
```


### Make the Symlink Tool

```{r instantiate_slt_prep_display}
# Instantiate (create) a new Symlink Tool object
slt_prep <- SLT$new(user_root_list = list("to_model" = root_to_model), user_central_log_root = root_to_model)
# Look at the directory tree
print_tree(root_to_model)
```

### New Folder

Use the Symlink Tool to create a new folder.  We're also using a helper function to auto-increments versions run on a certain date.

```{r baseline_folder_display_1}
date_vers1 <- get_output_dir(root_to_model, "2024_02_01")
slt_prep$create_date_version_folders_with_logs(date_version = date_vers1)
```

Capture some paths, using the Symlink Tool to help.  We'll use these in a minute.

```{r capture_paths_dv1}
# Capture some constants we'll use later
# NOTE: These can be drawn from within the Symlink Tool 
path_log_central <- slt_prep$return_dictionaries()$LOG_CENTRAL$path
root_dv1         <- slt_prep$return_dynamic_fields()$VERS_PATHS
fname_dv_log     <- slt_prep$return_dictionaries()$log_name
path_log_dv1     <- file.path(root_dv1, fname_dv_log)
```


Show the file tree.

```{r baseline_folder_display_2, echo=FALSE}
print_tree(root_to_model)
```

Show central log.

```{r baseline_folder_display_3, echo=FALSE}
(log_central <- fread(path_log_central))
```

Show new date_version folder log.

```{r baseline_folder_display_4, echo=FALSE}
(log_dv1 <- fread(path_log_dv1))
```



### Produce Model Results


Now let's make some files representing models in this folder.
Once we insepect the results, we can decide to mark the folder as 'best', or not.

```{r dummy_results_invis_1}
# Make some dummy files
(fnames_my_models <- paste0("my_model_", 1:5, ".csv"))
invisible(file.create(file.path(root_dv1, fnames_my_models)))
print_tree(root_to_model)
```



### Mark Best


Let's say we inspect the files, like the models, and want to elevate this date-versioned folder to 'best' status.

**NOTE:**  All `mark_xxxx` operations require a user entry as a named list.  

- Only a `comment` field is currently supported.

```{r mark_best_dv1}
# Mark best, and take note of messaging
slt_prep$mark_best(date_version = date_vers1, user_entry = list(comment = "Best model GBD2023"))
```


Inspect both the central log and the date-versioned folder log.

```{r mark_best_dv1_display_1, echo=FALSE}
(log_central <- fread(path_log_central))
```

```{r mark_best_dv1_display_2, echo=FALSE}
(log_dv1 <- fread(path_log_dv1))
```


And let's look at our file structure.  We now have a 'best' symlink that points to our 'best' ouput version, `2024_02_01.01`

```{r mark_best_dv1_display_3}
print_tree(root_to_model)
resolve_symlink(file.path(root_to_model, "best"))
```




### New Pipeline Runs

Of course we run our pipelines many times, and want to keep track of those runs.
We'll now produce two new pipeline 'runs', inspect the output, and make a human decision about the results quality.

- Make two new pipeline runs, each with dummy files.
- Use our helper function to increment pipeline output versions.

```{r two_new_runs}
# Second run
date_vers2 <- get_output_dir(root_to_model, "2024_02_01")
slt_prep$create_date_version_folders_with_logs(date_version = date_vers2)
# note - the dynamic fields update when you make new folders, so we won't see the dv1 path anymore
root_dv2 <- slt_prep$return_dynamic_fields()$VERS_PATHS
invisible(file.create(file.path(root_dv2, fnames_my_models)))

# Third run
date_vers3 <- get_output_dir(root_to_model, "2024_02_01")
slt_prep$create_date_version_folders_with_logs(date_version = date_vers3)
root_dv3 <- slt_prep$return_dynamic_fields()$VERS_PATHS
invisible(file.create(file.path(root_dv3, fnames_my_models)))
```




Now let's look at our file output structure, and central log.

- We see three total folders, and the central log hasn't changed yet, because we haven't done any more 'marking'.

```{r two_new_runs_display_1}
print_tree(root_to_model)
```

```{r two_new_runs_display_2, echo=FALSE}
(log_central <- fread(path_log_central))
```



### Mark New Best

After inspecting our results, we decide the third run is the best.  We'll mark it as such.

```{r mark_best_dv3}
# Mark best, and take note of messaging
slt_prep$mark_best(date_version = date_vers3, user_entry = list(comment = "New best model GBD2023"))
```

Inspect the central log - The third version is now bested.

```{r mark_best_dv3_display_1, echo=FALSE}
(log_central <- fread(path_log_central))
```



Let's also take a look inside each of the date-versioned folder logs.  They are actually the 'source of truth' for the folder status.  They have much more detail.  The central log is maintained with as little extra information as possible.

- We see the first output folder was demoted from `best` automatically, and the third version was marked as `best`.  
- This is also show in the file tree, and the `best` symlink points to the third pipeline run.

```{r mark_best_dv3_display_2, echo=FALSE}
(log_dv1 <- fread(path_log_dv1))
```

```{r mark_best_dv3_display_3, echo=FALSE}
(log_dv2 <- fread(file.path(root_dv2, fname_dv_log)))
```

```{r mark_best_dv3_display_4, echo=FALSE}
(log_dv3 <- fread(file.path(root_dv3, fname_dv_log)))
```

```{r mark_best_dv3_display_4}
print_tree(root_to_model)
resolve_symlink(file.path(root_to_model, "best"))
```
